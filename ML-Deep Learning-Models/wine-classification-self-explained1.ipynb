{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "26307496-31e7-4983-8925-b18c5e6e5eba",
    "_execution_state": "idle",
    "_uuid": "1b32e6d5aae17b0fdb0efc909561e60fb1de4646"
   },
   "source": [
    "## Introduction ##\n",
    "#### Welcome to this hands-on learning notebook.\n",
    "###### Here, we use our basic Machine learning models to classify a wine sample into three different categories(regions) based on its content.\n",
    "<img src=\"Images/Wine/1.png\"  width=\"300\" height=\"300\"><br>\n",
    "* These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines.\n",
    "* Our task is, given the contents of the wine, we have to classify it into the three regions as stated above<br><br>\n",
    "### Lets get started!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4906fc2e-5584-44f0-87d5-cf5e5dd09556",
    "_execution_state": "idle",
    "_uuid": "be7ffaa0b00c9143e99d76f50dcd22ec8a6f2781"
   },
   "source": [
    "# **Importing the base libraries : numpy,pandas,matplot**\n",
    "* numpy is used to perfoem almost any mathematical operations in python\n",
    "* pandas makes, reading, manipulating, summarizing, deleting the data\n",
    "* matplatlib is used to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "bb66538a-7444-4d49-a990-7ec18a2785b9",
    "_execution_state": "idle",
    "_uuid": "00e8e07a4346f813b91cf376a62ef22e6cc0e225"
   },
   "outputs": [],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d5790616-0101-44b7-aa03-2a08e314257c",
    "_execution_state": "idle",
    "_uuid": "9ae6d1fcebc482795c6ee2cbb6b74becb79077af"
   },
   "source": [
    "## Loading Data ##\n",
    " A DataFrame is a table. It contains an array of individual entries, each of which has a certain value. Each entry corresponds to a row (or record) and a column.\n",
    "\n",
    "For example, consider the following simple DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Yes</th>\n",
       "      <th>No</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Yes   No\n",
       "0   50  131\n",
       "1   21    2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'Yes': [50, 21], 'No': [131, 2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pandas has \"read_csv\" function to read the CSV file into a dataframe. We can have our own column names as shown below.\n",
    "* The head function gives first 'n' rows from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "cda6bfa2-ce98-4842-9ac0-f69fef35cf28",
    "_execution_state": "idle",
    "_uuid": "8179c2f5a2684938c75d96f7962cb92f489063e2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malicAcid</th>\n",
       "      <th>ash</th>\n",
       "      <th>ashalcalinity</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>totalPhenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonFlavanoidPhenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>colorIntensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280_od315</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   name  alcohol  malicAcid   ash  ashalcalinity  magnesium  totalPhenols  \\\n",
       "0     1    14.23       1.71  2.43           15.6        127          2.80   \n",
       "1     1    13.20       1.78  2.14           11.2        100          2.65   \n",
       "2     1    13.16       2.36  2.67           18.6        101          2.80   \n",
       "3     1    14.37       1.95  2.50           16.8        113          3.85   \n",
       "4     1    13.24       2.59  2.87           21.0        118          2.80   \n",
       "\n",
       "   flavanoids  nonFlavanoidPhenols  proanthocyanins  colorIntensity   hue  \\\n",
       "0        3.06                 0.28             2.29            5.64  1.04   \n",
       "1        2.76                 0.26             1.28            4.38  1.05   \n",
       "2        3.24                 0.30             2.81            5.68  1.03   \n",
       "3        3.49                 0.24             2.18            7.80  0.86   \n",
       "4        2.69                 0.39             1.82            4.32  1.04   \n",
       "\n",
       "   od280_od315  proline  \n",
       "0         3.92     1065  \n",
       "1         3.40     1050  \n",
       "2         3.17     1185  \n",
       "3         3.45     1480  \n",
       "4         2.93      735  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = [ 'name'\n",
    "             ,'alcohol'\n",
    "             ,'malicAcid'\n",
    "             ,'ash'\n",
    "            ,'ashalcalinity'\n",
    "             ,'magnesium'\n",
    "            ,'totalPhenols'\n",
    "             ,'flavanoids'\n",
    "             ,'nonFlavanoidPhenols'\n",
    "             ,'proanthocyanins'\n",
    "            ,'colorIntensity'\n",
    "             ,'hue'\n",
    "             ,'od280_od315'\n",
    "             ,'proline']\n",
    "\n",
    "df=pd.read_csv('Wine.csv',names=column_names)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5921cdb9-0316-4c3e-a206-152c593ea5e7",
    "_execution_state": "idle",
    "_uuid": "fbaf4feafb24221409e95af96778b654e530b8c6"
   },
   "source": [
    "## Check for Missing Values ##\n",
    "Entries missing values are given the value NaN, short for \"Not a Number\". For technical reasons these NaN values are always of the float64 dtype.\n",
    "\n",
    "* We have to check for missing values. If there are more missing values, then we have to remove that column, else, we can remove the corresponding rows.\n",
    "* We have a standard function \"isnull()\" to check whether a column has missing values.\n",
    "* It returns a binary list, each entry corresponding to a column.\n",
    " ex: [ 0, 1, 0, 1, 1, 1, 0, 0, 0 ]\n",
    "* We sum it to see how many columns have null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "9c369cc2-6e6b-497d-b816-8d2e7d303626",
    "_execution_state": "idle",
    "_uuid": "152e6e77aad214bc5bbca3f458c00a6f78e15a26"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name                   0\n",
       "alcohol                0\n",
       "malicAcid              0\n",
       "ash                    0\n",
       "ashalcalinity          0\n",
       "magnesium              0\n",
       "totalPhenols           0\n",
       "flavanoids             0\n",
       "nonFlavanoidPhenols    0\n",
       "proanthocyanins        0\n",
       "colorIntensity         0\n",
       "hue                    0\n",
       "od280_od315            0\n",
       "proline                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, We have no missing values in any column.\n",
    "* But if there are, Pandas provides a really handy method for this problem: fillna(). fillna() provides a few different strategies for mitigating such data. For example, we can simply replace each NaN with a 0<br>\n",
    "ex: df.alchohol.fillna(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      14.23\n",
       "1      13.20\n",
       "2      13.16\n",
       "3      14.37\n",
       "4      13.24\n",
       "       ...  \n",
       "173    13.71\n",
       "174    13.40\n",
       "175    13.27\n",
       "176    13.17\n",
       "177    14.13\n",
       "Name: alcohol, Length: 178, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.alcohol.fillna(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6aa1174e-4e48-4961-80ac-4e203db36fb2",
    "_execution_state": "idle",
    "_uuid": "2c69deafc5510b2c76aab1df4090494a9c45543b"
   },
   "source": [
    "## Features ##\n",
    "* Since the first column is the target variable for prediction, we drop it.\n",
    "* We store the remaining features into a dataframe 'X'\n",
    "* The \"drop()\" method is used for this which requires column and axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "9f3afd73-cbb6-42e8-9944-8d895218e471",
    "_execution_state": "idle",
    "_uuid": "a99dd4feb9319d1dfc44c653aa97865b08bea48f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malicAcid</th>\n",
       "      <th>ash</th>\n",
       "      <th>ashalcalinity</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>totalPhenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonFlavanoidPhenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>colorIntensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280_od315</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malicAcid   ash  ashalcalinity  magnesium  totalPhenols  \\\n",
       "0    14.23       1.71  2.43           15.6        127          2.80   \n",
       "1    13.20       1.78  2.14           11.2        100          2.65   \n",
       "2    13.16       2.36  2.67           18.6        101          2.80   \n",
       "3    14.37       1.95  2.50           16.8        113          3.85   \n",
       "4    13.24       2.59  2.87           21.0        118          2.80   \n",
       "\n",
       "   flavanoids  nonFlavanoidPhenols  proanthocyanins  colorIntensity   hue  \\\n",
       "0        3.06                 0.28             2.29            5.64  1.04   \n",
       "1        2.76                 0.26             1.28            4.38  1.05   \n",
       "2        3.24                 0.30             2.81            5.68  1.03   \n",
       "3        3.49                 0.24             2.18            7.80  0.86   \n",
       "4        2.69                 0.39             1.82            4.32  1.04   \n",
       "\n",
       "   od280_od315  proline  \n",
       "0         3.92     1065  \n",
       "1         3.40     1050  \n",
       "2         3.17     1185  \n",
       "3         3.45     1480  \n",
       "4         2.93      735  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X= df.drop(['name'], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index-based selection\n",
    "Pandas indexing works in one of two paradigms. The first is index-based selection: selecting data based on its numerical position in the data which is iloc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name                      1.00\n",
       "alcohol                  14.23\n",
       "malicAcid                 1.71\n",
       "ash                       2.43\n",
       "ashalcalinity            15.60\n",
       "magnesium               127.00\n",
       "totalPhenols              2.80\n",
       "flavanoids                3.06\n",
       "nonFlavanoidPhenols       0.28\n",
       "proanthocyanins           2.29\n",
       "colorIntensity            5.64\n",
       "hue                       1.04\n",
       "od280_od315               3.92\n",
       "proline                1065.00\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0] # df.iloc[\"row_num\",\"col_num\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5512f1dd-618c-4ed1-a764-42238119bb00",
    "_execution_state": "idle",
    "_uuid": "4948ece2bbb983e033c806e847d9262dae8f7aeb"
   },
   "source": [
    "## Labels ##\n",
    "* We need to extract the target variable with \"iloc\".\n",
    "* Finally, it's worth knowing that negative numbers can be used in selection. This will start counting forwards from the end of the values. So for example here are the last five elements of the dataset.\n",
    "* ':' stands for all,so we are selecting all rows or columns. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "56016780-98b6-4352-8660-bbf9c038deb5",
    "_execution_state": "idle",
    "_uuid": "a3be993b548ff858fc342562a1763b7312196523"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: name, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=df.iloc[:,0] #Selecting all rows from 1st column\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bbd4ad0e-118e-4e11-a6c3-39db4da5f6f8",
    "_execution_state": "idle",
    "_uuid": "d83efaa9a9d1bcb2a82f7bd5d2de94012dc3bc72"
   },
   "source": [
    "## Train-Test Split ##\n",
    "* The power of sklearn is unleashed here.\n",
    "* Here, it contains a handy function called \"train_test_split\" which helps us to divide the dataset into train and test.\n",
    "* We have to input the fraction we want to divide them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "739e615b-eef3-4676-bdf8-9afa3d4c31f8",
    "_execution_state": "idle",
    "_uuid": "2ad3fb7711ca45259fd5b092e7e591ee643b34c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124, 13)\n",
      "(54, 13)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "* There are some functions which are of standard use while we do classification<br>\n",
    "k-Fold Cross-Validation\n",
    "Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.<br><br>\n",
    "\n",
    "## 1) K-fold Cross Validation\n",
    "* Cross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general.<br>\n",
    "* The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation.\n",
    "<br>\n",
    "* It is a popular method and standard method used in selection of algorithms.\n",
    "<br><br>\n",
    "\n",
    "### Working <br>\n",
    "\n",
    "Imagine we have a data sample with 6 observations:\n",
    "<br>\n",
    " [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "<br>\n",
    "* The first step is to pick a value for k in order to determine the number of folds used to split the data. Here, we will use a value of k=3. \n",
    "* That means we will shuffle the data and then split the data into 3 groups. Because we have 6 observations, each group will have an equal number of 2 observations.<br><br>\n",
    "Fold1: [0.5, 0.2]<br>\n",
    "Fold2: [0.1, 0.3]<br>\n",
    "Fold3: [0.4, 0.6]\n",
    "<br><br>\n",
    "* Three models are trained and evaluated with each fold given a chance to be the held out test set.<br>\n",
    "<br>\n",
    "Model1: Trained on Fold1 + Fold2, Tested on Fold3<br>\n",
    "Model2: Trained on Fold2 + Fold3, Tested on Fold1<br>\n",
    "Model3: Trained on Fold1 + Fold3, Tested on Fold2\n",
    "<br><br>\n",
    "* The models are then discarded after they are evaluated as they have served their purpose.\n",
    "* The skill scores are collected for each model,summarized for use and the best average scored model is selected.\n",
    "\n",
    "### API <br>\n",
    "* We do not have to implement k-fold cross-validation manually. The scikit-learn library provides an implementation that will split a given data sample up.\n",
    "\n",
    "* The KFold() and cross_val_score() scikit-learn classes can be used. It takes as arguments the number of splits, whether or not to shuffle the sample, and the seed for the pseudorandom number generator used prior to the shuffle.<br>\n",
    "**ex: kfold = KFold(3, True, 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n"
     ]
    }
   ],
   "source": [
    "#sample code cell, do not run\n",
    "kfold = KFold(n_splits=10, random_state=0)\n",
    "cv_result = cross_val_score(model,X_train,Y_train.values.ravel(),cv = kfold,scoring = \"accuracy\")\n",
    "# returns the above defined score for given \"model\" and \"kfold\" partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "9f447300-91e3-4dd9-905f-2475d85a1dbc",
    "_execution_state": "idle",
    "_uuid": "692121dd311c89acca6e58d19a82605f039937d0"
   },
   "source": [
    "## Classifier Models ##\n",
    "* Here comes our Machine Learning models we have learnt. By applying a model, we are generally making rules from the train data.\n",
    "* We use these rules for future prediction. This is ML in a nutshell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELS\n",
    "* We use classification models to classify the type of wine.\n",
    "* But which model is the best. So, here comes the iterative process involved in Machine Learning.\n",
    "* In an itertative process, we try different models with different settings, and picks up the one which performs the best based on the above defined k-fold metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We take a list of models and its accuracies\n",
    "model_names = []\n",
    "model_accuracy = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL-1 (LOGISTIC REGRESSION) ###\n",
    "* This is the simplest classification algorithm we can use in ML.\n",
    "* It draws linear-like boundaries between classes two differenciate them.\n",
    "<img src=\"Images/Wine/2.png\"  width=\"300\" height=\"300\"><br>\n",
    "* It uses a function called sigmoid which squashes the values between 0 and 1 (like probability).\n",
    "* This can be extended to multiclass by taking one to many technique.\n",
    "* Abstracting us from the complexity, scikit learn gives us LogisticRegression() to achieve our objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : 0.9608974358974359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "kfold = KFold(n_splits=10, random_state=0,shuffle=True)\n",
    "cv_result = cross_val_score(model,X_train,Y_train, cv = kfold,scoring = \"accuracy\")\n",
    "model_names.append(\"Logistic_Regression\")\n",
    "model_accuracy.append(cv_result.mean())\n",
    "print(\"Logistic Regression :\",cv_result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Model-2 (K-Nearest-Neighbours) ###\n",
    "* * This is a very intuitive and easy method for classification.\n",
    "* A case is classified by a majority vote of its neighbors, with the case being assigned to the class most common amongst its K nearest neighbors measured by a distance function.\n",
    "* Eucledean distance is mostly used as a distance function.<br><br>\n",
    "<img src=\"Images/Wine/3.png\"  width=\"400\" height=\"400\"><br>\n",
    "* Scikit Learn provides an abstract function \"KNeighborsClassifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest-Neighbors : 0.7108974358974358\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "kfold = KFold(n_splits=10, random_state=0,shuffle=True)\n",
    "cv_result = cross_val_score(model,X_train,Y_train, cv = kfold,scoring = \"accuracy\")\n",
    "model_names.append(\"K-Nearest-Neighbors\")\n",
    "model_accuracy.append(cv_result.mean())\n",
    "print(\"K-Nearest-Neighbors :\",cv_result.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Model-3 (Decision Trees) ###\n",
    " * Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. \n",
    " * The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    " * Here is an example :\n",
    "<img src=\"Images/Wine/4.png\"  width=\"300\" height=\"300\"><br>\n",
    "* This is most exactly what decision tree does.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Trees : 0.9198717948717947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "kfold = KFold(n_splits=10, random_state=0,shuffle=True)\n",
    "cv_result = cross_val_score(model,X_train,Y_train, cv = kfold,scoring = \"accuracy\")\n",
    "model_names.append(\"Decision Trees\")\n",
    "model_accuracy.append(cv_result.mean())\n",
    "print(\"Decision Trees :\",cv_result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Model-4 (Random Forest Classifier) ###\n",
    " * This is one of the best classifiers presently in use which has a concept of ensembling.\n",
    " * Ensembling is a process of combining multiple models, train them parallely, and select the most suitable outcome.\n",
    "* The core working of this model is same as decision trees, except that many trees are constructed and the most voted output is considered.<br>\n",
    "<img src=\"Images/Wine/5.png\"  width=\"300\" height=\"300\"><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest : 0.9673076923076922\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators = 10)\n",
    "kfold = KFold(n_splits=10, random_state=0,shuffle=True)\n",
    "cv_result = cross_val_score(model,X_train,Y_train, cv = kfold,scoring = \"accuracy\")\n",
    "model_names.append(\"Random Forest\")\n",
    "model_accuracy.append(cv_result.mean())\n",
    "print(\"Random Forest :\",cv_result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Model-4 (Support Vector Machines) ###\n",
    " * Support vector Machines are robust classifiers.\n",
    " * SVC's divide the entire space by hyperplanes(high dimensional lines) to divide each class.\n",
    " * Additional to that, it maintains the best seperation between the classes by maintaining large possible distance from the near points(support vectors).<br>\n",
    "<img src=\"Images/Wine/6.png\"  width=\"300\" height=\"300\"><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Classifier : 0.9596153846153845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC( kernel = \"linear\") #linear hyperplane, we can also choose non-linear\n",
    "kfold = KFold(n_splits=10, random_state=0,shuffle=True)\n",
    "cv_result = cross_val_score(model,X_train,Y_train, cv = kfold,scoring = \"accuracy\")\n",
    "model_names.append(\"Support Vector Classifier\")\n",
    "model_accuracy.append(cv_result.mean())\n",
    "print(\"Support Vector Classifier :\",cv_result.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "91ea968a-f338-4683-b710-4fd17741231c",
    "_execution_state": "idle",
    "_uuid": "0780fceff50e8c16a627c6a9d31ad780539b5674"
   },
   "source": [
    "## Results ##\n",
    "* This is the summary of all the models we have used along with its k-fold accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "_cell_guid": "5feb9192-c27b-4406-a591-4a7be50cf293",
    "_execution_state": "idle",
    "_uuid": "1f63a2db23a8e9013d9dc6f3b948a2b914d5f754"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic_Regression : 0.9608974358974359\n",
      "K-Nearest-Neighbors : 0.7108974358974358\n",
      "Decision Trees : 0.9198717948717947\n",
      "Random Forest : 0.9673076923076922\n",
      "Support Vector Classifier : 0.9596153846153845\n"
     ]
    }
   ],
   "source": [
    "num_of_models = len(model_names)\n",
    "for i in range(num_of_models):\n",
    "    print(model_names[i],':',model_accuracy[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_execution_state": "idle",
    "_uuid": "efee12af11d890a5216b060e7e9be07afbfeb1fe"
   },
   "source": [
    "# Thank You\n",
    "* There is a lot of feature engineering which is not required here because of the clean data.\n",
    "* This is a very basic try of all models without much optimization and hyperparameter tuning. \n",
    "* Results of accuracy over 95% for some top models but not always the caseIt is worth trying few models like these to get better models. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
